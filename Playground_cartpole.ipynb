{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Critic experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from envs.gridworld import GridworldEnv\n",
    "from envs.windy_gridworld import WindyGridworldEnv\n",
    "from models import PolicyNetwork, ValueNetwork\n",
    "from utils import (compare_baselines_plot, get_running_time, plot_episodes_durations_losses, run_episode,\n",
    "                   sample_greedy_return, select_action, set_seeds, smooth)\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "EPS = float(np.finfo(np.float32).eps)\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 50\n",
    "num_episodes = 1000\n",
    "discount_factor = 0.99\n",
    "learn_rate = 0.001\n",
    "grid_shape = [10, 10]\n",
    "init_temperature = 1.1\n",
    "stochasticity = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE w/ No Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reinforce_loss_no_baseline(episode, discount_factor):\n",
    "    discounted_return_list = []\n",
    "    log_p_list = []\n",
    "    G = 0\n",
    "    \n",
    "    for s, a, log_p, s_next, reward in reversed(episode):\n",
    "        G = reward + discount_factor * G\n",
    "        \n",
    "        discounted_return_list.append(G)\n",
    "        log_p_list.append(log_p)\n",
    "    \n",
    "    log_p_tensor = torch.stack(log_p_list)\n",
    "    discounted_return_tensor = torch.FloatTensor(discounted_return_list)\n",
    "    \n",
    "    loss = - torch.sum(log_p_tensor * discounted_return_tensor)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def run_episodes_no_baseline(model, env, num_episodes, discount_factor, learn_rate, init_temp = init_temperature): \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    episode_durations = []\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        episode = run_episode(env, model, i, init_temp, stochasticity)\n",
    "        loss = compute_reinforce_loss_no_baseline(episode, discount_factor)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "          \n",
    "        losses.append(loss.detach().numpy())\n",
    "        episode_durations.append(len(episode))\n",
    "    \n",
    "        del episode\n",
    "        \n",
    "    return np.asanyarray(episode_durations), np.asanyarray(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_nobaseline(param_dict):\n",
    "    print(param_dict)\n",
    "    env = gym.make('CartPole-v1')\n",
    "    opt_num_runs = 5\n",
    "    result = 0\n",
    "    for i in range(opt_num_runs):\n",
    "        start_time = time()\n",
    "\n",
    "        model = PolicyNetwork(input_dim=4, hidden_dim=int(param_dict['policy_hiddens']), output_dim=2)\n",
    "        seed = 40 + i\n",
    "        set_seeds(env, seed)\n",
    "\n",
    "        episode_durations, _ = run_episodes_no_baseline(model, \n",
    "                                                        env, \n",
    "                                                        num_episodes, \n",
    "                                                        param_dict['discount_factor'], \n",
    "                                                        param_dict['learn_rate'],\n",
    "                                                        init_temp = param_dict['init_temp'])\n",
    "        result += sum(episode_durations)/len(episode_durations)\n",
    "\n",
    "        del model\n",
    "    \n",
    "    result /= opt_num_runs\n",
    "    print(\"average number of episodes:\", result)\n",
    "    return result\n",
    "\n",
    "space = {\n",
    "    'discount_factor': hp.uniform('discount_factor', 0.000001, 0.9999999),\n",
    "    'learn_rate': hp.loguniform('learn_rate', -5, -1),\n",
    "    'policy_hiddens': hp.quniform('policy_hiddens', 16, 512, 2),\n",
    "    'init_temp': hp.uniform('init_temp', 1, 2),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=opt_nobaseline, space=space, algo=tpe.suggest, max_evals=25, trials=trials)\n",
    "print(\"best run:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "no_episode_durations = []\n",
    "no_policy_losses = []\n",
    "\n",
    "# hyper parameter settings found through tuning\n",
    "discount_factor = 0.7919018685923426\n",
    "init_temp = 1.018128600121523\n",
    "learn_rate = 0.21449972356736796\n",
    "hidden_dim = 28\n",
    "\n",
    "\n",
    "for i in range(num_runs):\n",
    "    start_time = time()\n",
    "\n",
    "    model = PolicyNetwork(input_dim=4, hidden_dim=hidden_dim, output_dim=2)\n",
    "    seed = 40 + i\n",
    "    set_seeds(env, seed)\n",
    "\n",
    "    episode_durations, policy_losses = run_episodes_no_baseline(model, \n",
    "                                                                env, \n",
    "                                                                num_episodes, \n",
    "                                                                discount_factor, \n",
    "                                                                learn_rate,\n",
    "                                                                init_temp)\n",
    "    \n",
    "    no_episode_durations.append(episode_durations)\n",
    "    no_policy_losses.append(policy_losses)\n",
    "    \n",
    "    del model\n",
    "    \n",
    "    end_time = time()\n",
    "    h, m, s = get_running_time(end_time - start_time)\n",
    "    \n",
    "    print(f'Done with run {i+1}/{num_runs} in {f\"{h} hours, \" if h else \"\"}{f\"{m} minutes and \" if m else \"\"}{s} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episodes_durations_losses(no_episode_durations, no_policy_losses, None, 'No baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE w/ Learned Baseline (Value Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reinforce_loss_with_learned_baseline(value_model, episode, discount_factor, env):    \n",
    "    discounted_return_list = []\n",
    "    log_p_list = []\n",
    "    G = 0\n",
    "    \n",
    "    for s, a, log_p, s_next, reward in reversed(episode):\n",
    "        G = reward + discount_factor * G\n",
    "        \n",
    "        # state = np.unravel_index(s, env.shape)\n",
    "        baseline = value_model(torch.FloatTensor(s))\n",
    "        \n",
    "        discounted_return_list.append(G - baseline)\n",
    "        log_p_list.append(log_p)\n",
    "        \n",
    "    log_p_tensor = torch.stack(log_p_list)\n",
    "    discounted_return_tensor = torch.FloatTensor(discounted_return_list)\n",
    "    \n",
    "    loss = - torch.sum(log_p_tensor * discounted_return_tensor)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compute_value_loss(value_model, episode, discount_factor, env):\n",
    "    returns = []\n",
    "    value_estimates = []\n",
    "    G = 0\n",
    "    \n",
    "    for s, a, log_p, s_next, reward in reversed(episode):\n",
    "        G = reward + discount_factor * G\n",
    "        returns.append(G)\n",
    "        \n",
    "        # state = np.unravel_index(s, env.shape)\n",
    "        value_estimates.append(value_model(torch.FloatTensor(s)))\n",
    "\n",
    "    value_estimates_tensor = torch.stack(value_estimates) \n",
    "    returns_tensor = torch.FloatTensor(returns)\n",
    "    \n",
    "    loss = torch.sum(torch.abs(returns_tensor - value_estimates_tensor))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def run_episodes_with_learned_baseline(policy_model, value_model, env, num_episodes, discount_factor, \n",
    "                                       learn_rate_policy, learn_rate_value, init_temp = init_temperature):\n",
    "    policy_optimizer = optim.Adam(policy_model.parameters(), learn_rate_policy)\n",
    "    value_optimizer = optim.Adam(value_model.parameters(), learn_rate_value)\n",
    "    \n",
    "    episode_durations = []\n",
    "    value_losses = []\n",
    "    reinforce_losses = []\n",
    "    \n",
    "    for i in range(num_episodes):    \n",
    "        policy_optimizer.zero_grad()\n",
    "        value_optimizer.zero_grad()\n",
    "        \n",
    "        episode = run_episode(env, policy_model, i, init_temp, stochasticity)\n",
    "        reinforce_loss = compute_reinforce_loss_with_learned_baseline(value_model, episode, discount_factor, env)\n",
    "        value_loss = compute_value_loss(value_model, episode, discount_factor, env)\n",
    "        \n",
    "        reinforce_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "            \n",
    "        episode_durations.append(len(episode))\n",
    "        reinforce_losses.append(reinforce_loss.detach().numpy())\n",
    "        value_losses.append(value_loss.detach().numpy())\n",
    "    \n",
    "        del episode\n",
    "        \n",
    "    return np.asanyarray(episode_durations), np.asanyarray(reinforce_losses), np.asanyarray(value_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_learned_baseline(param_dict):\n",
    "    print(param_dict)\n",
    "    env = gym.make('CartPole-v1')\n",
    "    opt_num_runs = 5\n",
    "    result = 0\n",
    "    for i in range(opt_num_runs):\n",
    "        start_time = time()\n",
    "\n",
    "        policy_model = PolicyNetwork(input_dim=4, hidden_dim = int(param_dict['policy_hiddens']), output_dim=2)\n",
    "        value_model = ValueNetwork(input_dim=4, hidden_dim = int(param_dict['value_hiddens']))\n",
    "        seed = 40 + i\n",
    "        set_seeds(env, seed)\n",
    "\n",
    "        episode_durations, _, _ = run_episodes_with_learned_baseline(policy_model,\n",
    "                                                                     value_model,\n",
    "                                                                     env,\n",
    "                                                                     num_episodes,\n",
    "                                                                     param_dict['discount_factor'], \n",
    "                                                                     param_dict['learn_rate_policy'],\n",
    "                                                                     param_dict['learn_rate_value'],\n",
    "                                                                     init_temp = param_dict['init_temp'],)\n",
    "        result += sum(episode_durations)/len(episode_durations)\n",
    "\n",
    "        del policy_model\n",
    "        del value_model\n",
    "    \n",
    "    result /= opt_num_runs\n",
    "    print(\"average number of episodes:\", result)\n",
    "    return result\n",
    "\n",
    "space = {\n",
    "    'discount_factor': hp.uniform('discount_factor', 0.000001, 0.9999999),\n",
    "    'learn_rate_policy': hp.loguniform('learn_rate_policy', -5, -1),\n",
    "    'learn_rate_value': hp.loguniform('learn_rate_value', -5, -1),\n",
    "    'policy_hiddens': hp.quniform('policy_hiddens', 16, 512, 2),\n",
    "    'value_hiddens': hp.quniform('value_hiddens', 16, 512, 2),\n",
    "    'init_temp': hp.uniform('init_temp', 1, 2),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=opt_learned_baseline, space=space, algo=tpe.suggest, max_evals=25, trials=trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "learned_baseline_episode_durations = []\n",
    "learned_baseline_policy_losses = []\n",
    "learned_baseline_value_losses = []\n",
    "\n",
    "# hyper parameter settings found through tuning\n",
    "discount_factor = 0.9067307370442468\n",
    "init_temp = 1.9831668926885457\n",
    "learn_rate_policy = 0.3266448140250817\n",
    "learn_rate_value = 0.13129023566325262\n",
    "hidden_dim_policy = 510\n",
    "hidden_dim_value = 432\n",
    "\n",
    "for i in range(num_runs):\n",
    "    start_time = time()\n",
    "    \n",
    "    policy_model = PolicyNetwork(input_dim=4, hidden_dim=hidden_dim_policy, output_dim=2)\n",
    "    value_model = ValueNetwork(input_dim=4, hidden_dim=hidden_dim_value)\n",
    "    seed = 40 + i\n",
    "    set_seeds(env, seed)\n",
    "\n",
    "    episode_durations, policy_losses, value_losses = run_episodes_with_learned_baseline(policy_model,\n",
    "                                                                                        value_model,\n",
    "                                                                                        env,\n",
    "                                                                                        num_episodes,\n",
    "                                                                                        discount_factor,\n",
    "                                                                                        learn_rate_policy,\n",
    "                                                                                        learn_rate_value,\n",
    "                                                                                        init_temp)\n",
    "    \n",
    "    learned_baseline_episode_durations.append(episode_durations)\n",
    "    learned_baseline_policy_losses.append(policy_losses)\n",
    "    learned_baseline_value_losses.append(value_losses)\n",
    "    \n",
    "    del policy_model\n",
    "    del value_model\n",
    "    \n",
    "    end_time = time()\n",
    "    h, m, s = get_running_time(end_time - start_time)\n",
    "    \n",
    "    print(f'Done with run {i+1}/{num_runs} in {f\"{h} hours, \" if h else \"\"}{f\"{m} minutes and \" if m else \"\"}{s} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episodes_durations_losses(learned_baseline_episode_durations, learned_baseline_policy_losses, learned_baseline_value_losses, 'Learned baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE w/ Self-Critic Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reinforce_loss_with_SC_baseline(model, episode, discount_factor, env):    \n",
    "    discounted_return_list = []\n",
    "    log_p_list = []\n",
    "    G = 0\n",
    "    \n",
    "    for s, a, log_p, s_next, reward in reversed(episode):\n",
    "        G = reward + discount_factor * G\n",
    "        \n",
    "        baseline = sample_greedy_return(model, env, discount_factor, s)\n",
    "        \n",
    "        discounted_return_list.append(G - baseline)\n",
    "        log_p_list.append(log_p)\n",
    "        \n",
    "    log_p_tensor = torch.stack(log_p_list)\n",
    "    discounted_return_tensor = torch.FloatTensor(discounted_return_list)\n",
    "    \n",
    "    loss = - torch.sum(log_p_tensor * discounted_return_tensor)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def run_episodes_with_SC_baseline(model, env, num_episodes, discount_factor, learn_rate, init_temp = init_temperature):\n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    episode_durations = []\n",
    "    policy_losses = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        episode = run_episode(env, model, i, init_temp, stochasticity)\n",
    "        loss = compute_reinforce_loss_with_SC_baseline(model, episode, discount_factor, env)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        episode_durations.append(len(episode))\n",
    "        policy_losses.append(loss.detach().numpy())\n",
    "        \n",
    "        del episode\n",
    "        \n",
    "    return np.asanyarray(episode_durations), np.asanyarray(policy_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'discount_factor': 0.9990030370482987, 'init_temp': 1.0623639255845339, 'learn_rate_policy': 0.04699806551504531, 'policy_hiddens': 50.0}\n",
      "average number of episodes:                                                                                            \n",
      "36.6546                                                                                                                \n",
      "{'discount_factor': 0.9813725603397145, 'init_temp': 1.1173355838842431, 'learn_rate_policy': 0.05530423927049258, 'policy_hiddens': 400.0}\n",
      "average number of episodes:                                                                                            \n",
      "10.345600000000001                                                                                                     \n",
      "{'discount_factor': 0.9822410077379976, 'init_temp': 1.250339814041185, 'learn_rate_policy': 0.2031167875849609, 'policy_hiddens': 86.0}\n",
      "average number of episodes:                                                                                            \n",
      "9.3652                                                                                                                 \n",
      "{'discount_factor': 0.9783091666181833, 'init_temp': 1.2843458148955351, 'learn_rate_policy': 0.23070993825690678, 'policy_hiddens': 478.0}\n",
      "average number of episodes:                                                                                            \n",
      "9.3798                                                                                                                 \n",
      "{'discount_factor': 0.9768847645744314, 'init_temp': 1.06608233064019, 'learn_rate_policy': 0.3023013565766567, 'policy_hiddens': 484.0}\n",
      "average number of episodes:                                                                                            \n",
      "9.620999999999999                                                                                                      \n",
      "{'discount_factor': 0.9747103847875972, 'init_temp': 1.053023786167285, 'learn_rate_policy': 0.012722121872356712, 'policy_hiddens': 276.0}\n",
      " 20%|████████████▏                                                | 5/25 [35:28<2:32:49, 458.45s/it, best loss: 9.3652]"
     ]
    }
   ],
   "source": [
    "def opt_SC_baseline(param_dict):\n",
    "    print(param_dict)\n",
    "    env = gym.make('CartPole-v1')\n",
    "    opt_num_runs = 5\n",
    "    result = 0\n",
    "        \n",
    "    for i in range(opt_num_runs):\n",
    "        start_time = time()\n",
    "\n",
    "        policy_model = PolicyNetwork(input_dim=4, hidden_dim = int(param_dict['policy_hiddens']), output_dim=2)\n",
    "        seed = 40 + i\n",
    "        set_seeds(env, seed)\n",
    "\n",
    "        episode_durations, _ = run_episodes_with_SC_baseline(policy_model,\n",
    "                                                            env,\n",
    "                                                            num_episodes,\n",
    "                                                            param_dict['discount_factor'], \n",
    "                                                            param_dict['learn_rate_policy'],\n",
    "                                                            init_temp = param_dict['init_temp'],)    \n",
    "        result += sum(episode_durations)/len(episode_durations)\n",
    "\n",
    "        del policy_model\n",
    "    \n",
    "    result /= opt_num_runs\n",
    "    print(\"average number of episodes:\", result)\n",
    "    return result\n",
    "\n",
    "space = {\n",
    "    'discount_factor': hp.uniform('discount_factor', 0.97, 0.9999999),\n",
    "    'learn_rate_policy': hp.loguniform('learn_rate_policy', -5, -1),\n",
    "    'policy_hiddens': hp.quniform('policy_hiddens', 16, 512, 2),\n",
    "    'init_temp': hp.uniform('init_temp', 1, 1.3),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=opt_SC_baseline, space=space, algo=tpe.suggest, max_evals=25, trials=trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "sc_baseline_episode_durations = []\n",
    "sc_baseline_policy_losses = []\n",
    "\n",
    "# hyper parameter settings found through tuning\n",
    "discount_factor = 0.9067307370442468\n",
    "init_temp = 1.9831668926885457\n",
    "learn_rate = 0.3266448140250817\n",
    "hidden_dim = 510\n",
    "\n",
    "for i in range(num_runs):\n",
    "    start_time = time()\n",
    "    \n",
    "    policy_model = PolicyNetwork(input_dim=4, hidden_dim =hidden_dim, output_dim=2)\n",
    "    seed = 40 + i\n",
    "    set_seeds(env, seed)\n",
    "\n",
    "    episode_durations, policy_losses = run_episodes_with_SC_baseline(policy_model, \n",
    "                                                                     env, \n",
    "                                                                     num_episodes, \n",
    "                                                                     discount_factor, \n",
    "                                                                     learn_rate)\n",
    "    \n",
    "    sc_baseline_episode_durations.append(episode_durations)\n",
    "    sc_baseline_policy_losses.append(policy_losses)\n",
    "    \n",
    "    del policy_model\n",
    "    \n",
    "    end_time = time()\n",
    "    h, m, s = get_running_time(end_time - start_time)\n",
    "    \n",
    "    print(f'Done with run {i+1}/{num_runs} in {f\"{h} hours, \" if h else \"\"}{f\"{m} minutes and \" if m else \"\"}{s} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episodes_durations_losses(sc_baseline_episode_durations, sc_baseline_policy_losses, None, 'Self-Critic baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines_dict = {\n",
    "    'No baseline': {\n",
    "        'episode_durations': no_episode_durations,\n",
    "        'policy_losses': no_policy_losses,\n",
    "        'color': 'red'\n",
    "    }, 'Learned baseline': {\n",
    "        'episode_durations': learned_baseline_episode_durations,\n",
    "        'policy_losses': learned_baseline_policy_losses,\n",
    "        'color': 'green'\n",
    "    }, 'Self-Critic baseline': {\n",
    "        'episode_durations': sc_baseline_episode_durations,\n",
    "        'policy_losses': sc_baseline_policy_losses,\n",
    "        'color': 'blue'\n",
    "    }\n",
    "}\n",
    "\n",
    "compare_baselines_plot(baselines_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

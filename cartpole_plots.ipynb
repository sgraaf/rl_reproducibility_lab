{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Critic experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from envs.gridworld import GridworldEnv\n",
    "from envs.windy_gridworld import WindyGridworldEnv\n",
    "from models import PolicyNetwork, ValueNetwork\n",
    "from utils import (compare_baselines_plot, get_running_time, plot_episodes_durations_losses, run_episode,\n",
    "                   sample_greedy_return, select_action, set_seeds, smooth)\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "EPS = float(np.finfo(np.float32).eps)\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 20\n",
    "num_episodes = 750\n",
    "discount_factor = 0.99\n",
    "learn_rate = 0.001\n",
    "grid_shape = [10, 10]\n",
    "init_temp = 1.1\n",
    "stochasticity = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE w/ No Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reinforce_loss_no_baseline(episode, discount_factor):\n",
    "    discounted_return_list = []\n",
    "    log_p_list = []\n",
    "    G = 0\n",
    "    \n",
    "    for s, a, log_p, s_next, reward in reversed(episode):\n",
    "        G = reward + discount_factor * G\n",
    "        \n",
    "        discounted_return_list.append(G)\n",
    "        log_p_list.append(log_p)\n",
    "    \n",
    "    log_p_tensor = torch.stack(log_p_list)\n",
    "    discounted_return_tensor = torch.FloatTensor(discounted_return_list)\n",
    "    \n",
    "    loss = - torch.sum(log_p_tensor * discounted_return_tensor)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def run_episodes_no_baseline(model, env, num_episodes, discount_factor, learn_rate, init_temp = init_temperature): \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    episode_durations = []\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        episode = run_episode(env, model, i, init_temp, stochasticity)\n",
    "        loss = compute_reinforce_loss_no_baseline(episode, discount_factor)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "          \n",
    "        losses.append(loss.detach().numpy())\n",
    "        episode_durations.append(len(episode))\n",
    "    \n",
    "        del episode\n",
    "        \n",
    "    return np.asanyarray(episode_durations), np.asanyarray(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with run 1/20 in 1 minutes and 20 seconds\n",
      "Done with run 2/20 in 1 minutes and 16 seconds\n",
      "Done with run 3/20 in 1 minutes and 20 seconds\n",
      "Done with run 4/20 in 1 minutes and 22 seconds\n",
      "Done with run 5/20 in 1 minutes and 20 seconds\n",
      "Done with run 6/20 in 1 minutes and 23 seconds\n",
      "Done with run 7/20 in 1 minutes and 20 seconds\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "no_episode_durations = []\n",
    "no_policy_losses = []\n",
    "\n",
    "for i in range(num_runs):\n",
    "    start_time = time()\n",
    "\n",
    "    model = PolicyNetwork(input_dim=4, hidden_dim=128, output_dim=2)\n",
    "    seed = 40 + i\n",
    "    set_seeds(env, seed)\n",
    "\n",
    "    episode_durations, policy_losses = run_episodes_no_baseline(model, \n",
    "                                                                env, \n",
    "                                                                num_episodes, \n",
    "                                                                discount_factor, \n",
    "                                                                learn_rate,\n",
    "                                                                init_temp)\n",
    "    \n",
    "    no_episode_durations.append(episode_durations)\n",
    "    no_policy_losses.append(policy_losses)\n",
    "    \n",
    "    del model\n",
    "    \n",
    "    end_time = time()\n",
    "    h, m, s = get_running_time(end_time - start_time)\n",
    "    \n",
    "    print(f'Done with run {i+1}/{num_runs} in {f\"{h} hours, \" if h else \"\"}{f\"{m} minutes and \" if m else \"\"}{s} seconds')\n",
    "          \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episodes_durations_losses(no_episode_durations, no_policy_losses, None, 'No baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE w/ Learned Baseline (Value Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reinforce_loss_with_learned_baseline(value_model, episode, discount_factor, env):    \n",
    "    discounted_return_list = []\n",
    "    log_p_list = []\n",
    "    G = 0\n",
    "    \n",
    "    for s, a, log_p, s_next, reward in reversed(episode):\n",
    "        G = reward + discount_factor * G\n",
    "        \n",
    "        # state = np.unravel_index(s, env.shape)\n",
    "        baseline = value_model(torch.FloatTensor(s))\n",
    "        \n",
    "        discounted_return_list.append(G - baseline)\n",
    "        log_p_list.append(log_p)\n",
    "        \n",
    "    log_p_tensor = torch.stack(log_p_list)\n",
    "    discounted_return_tensor = torch.FloatTensor(discounted_return_list)\n",
    "    \n",
    "    loss = - torch.sum(log_p_tensor * discounted_return_tensor)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compute_value_loss(value_model, episode, discount_factor, env):\n",
    "    returns = []\n",
    "    value_estimates = []\n",
    "    G = 0\n",
    "    \n",
    "    for s, a, log_p, s_next, reward in reversed(episode):\n",
    "        G = reward + discount_factor * G\n",
    "        returns.append(G)\n",
    "        \n",
    "        # state = np.unravel_index(s, env.shape)\n",
    "        value_estimates.append(value_model(torch.FloatTensor(s)))\n",
    "\n",
    "    value_estimates_tensor = torch.stack(value_estimates) \n",
    "    returns_tensor = torch.FloatTensor(returns)\n",
    "    \n",
    "    loss = torch.sum(torch.abs(returns_tensor - value_estimates_tensor))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def run_episodes_with_learned_baseline(policy_model, value_model, env, num_episodes, discount_factor, \n",
    "                                       learn_rate_policy, learn_rate_value, init_temp = init_temperature):\n",
    "    policy_optimizer = optim.Adam(policy_model.parameters(), learn_rate_policy)\n",
    "    value_optimizer = optim.Adam(value_model.parameters(), learn_rate_value)\n",
    "    \n",
    "    episode_durations = []\n",
    "    value_losses = []\n",
    "    reinforce_losses = []\n",
    "    \n",
    "    for i in range(num_episodes):    \n",
    "        policy_optimizer.zero_grad()\n",
    "        value_optimizer.zero_grad()\n",
    "        \n",
    "        episode = run_episode(env, policy_model, i, init_temp, stochasticity)\n",
    "        reinforce_loss = compute_reinforce_loss_with_learned_baseline(value_model, episode, discount_factor, env)\n",
    "        value_loss = compute_value_loss(value_model, episode, discount_factor, env)\n",
    "        \n",
    "        reinforce_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "            \n",
    "        episode_durations.append(len(episode))\n",
    "        reinforce_losses.append(reinforce_loss.detach().numpy())\n",
    "        value_losses.append(value_loss.detach().numpy())\n",
    "    \n",
    "        del episode\n",
    "        \n",
    "    return np.asanyarray(episode_durations), np.asanyarray(reinforce_losses), np.asanyarray(value_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "learned_baseline_episode_durations = []\n",
    "learned_baseline_policy_losses = []\n",
    "learned_baseline_value_losses = []\n",
    "\n",
    "for i in range(num_runs):\n",
    "    start_time = time()\n",
    "    \n",
    "    policy_model = PolicyNetwork(input_dim=4, hidden_dim=hidden_dim, output_dim=2)\n",
    "    value_model = ValueNetwork(input_dim=4, hidden_dim=hidden_dim)\n",
    "    seed = 40 + i\n",
    "    set_seeds(env, seed)\n",
    "\n",
    "    episode_durations, policy_losses, value_losses = run_episodes_with_learned_baseline(policy_model,\n",
    "                                                                                        value_model,\n",
    "                                                                                        env,\n",
    "                                                                                        num_episodes,\n",
    "                                                                                        discount_factor,\n",
    "                                                                                        learn_rate,\n",
    "                                                                                        learn_rate,\n",
    "                                                                                        init_temp)\n",
    "    \n",
    "    learned_baseline_episode_durations.append(episode_durations)\n",
    "    learned_baseline_policy_losses.append(policy_losses)\n",
    "    learned_baseline_value_losses.append(value_losses)\n",
    "    \n",
    "    del policy_model\n",
    "    del value_model\n",
    "    \n",
    "    end_time = time()\n",
    "    h, m, s = get_running_time(end_time - start_time)\n",
    "    \n",
    "    print(f'Done with run {i+1}/{num_runs} in {f\"{h} hours, \" if h else \"\"}{f\"{m} minutes and \" if m else \"\"}{s} seconds')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episodes_durations_losses(learned_baseline_episode_durations, learned_baseline_policy_losses, learned_baseline_value_losses, 'Learned baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE w/ Self-Critic Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reinforce_loss_with_SC_baseline(model, episode, discount_factor, env):    \n",
    "    discounted_return_list = []\n",
    "    log_p_list = []\n",
    "    G = 0\n",
    "    \n",
    "    for s, a, log_p, s_next, reward in reversed(episode):\n",
    "        G = reward + discount_factor * G\n",
    "        \n",
    "        baseline = sample_greedy_return(model, env, discount_factor, s)\n",
    "        \n",
    "        discounted_return_list.append(G - baseline)\n",
    "        log_p_list.append(log_p)\n",
    "        \n",
    "    log_p_tensor = torch.stack(log_p_list)\n",
    "    discounted_return_tensor = torch.FloatTensor(discounted_return_list)\n",
    "    \n",
    "    loss = - torch.sum(log_p_tensor * discounted_return_tensor)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def run_episodes_with_SC_baseline(model, env, num_episodes, discount_factor, learn_rate, init_temp = init_temperature):\n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    episode_durations = []\n",
    "    policy_losses = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        episode = run_episode(env, model, i, init_temp, stochasticity)\n",
    "        loss = compute_reinforce_loss_with_SC_baseline(model, episode, discount_factor, env)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        episode_durations.append(len(episode))\n",
    "        policy_losses.append(loss.detach().numpy())\n",
    "        \n",
    "        del episode\n",
    "        \n",
    "    return np.asanyarray(episode_durations), np.asanyarray(policy_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "sc_baseline_episode_durations = []\n",
    "sc_baseline_policy_losses = []\n",
    "\n",
    "for i in range(num_runs):\n",
    "    start_time = time()\n",
    "    \n",
    "    policy_model = PolicyNetwork(input_dim=4, hidden_dim =hidden_dim, output_dim=2)\n",
    "    seed = 40 + i\n",
    "    set_seeds(env, seed)\n",
    "\n",
    "    episode_durations, policy_losses = run_episodes_with_SC_baseline(policy_model, \n",
    "                                                                     env, \n",
    "                                                                     num_episodes, \n",
    "                                                                     discount_factor, \n",
    "                                                                     learn_rate,\n",
    "                                                                     init_temp)\n",
    "    \n",
    "    sc_baseline_episode_durations.append(episode_durations)\n",
    "    sc_baseline_policy_losses.append(policy_losses)\n",
    "    \n",
    "    del policy_model\n",
    "    \n",
    "    end_time = time()\n",
    "    h, m, s = get_running_time(end_time - start_time)\n",
    "    \n",
    "    print(f'Done with run {i+1}/{num_runs} in {f\"{h} hours, \" if h else \"\"}{f\"{m} minutes and \" if m else \"\"}{s} seconds')\n",
    "          \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episodes_durations_losses(sc_baseline_episode_durations, sc_baseline_policy_losses, None, 'Self-Critic baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines_dict = {\n",
    "    'No baseline': {\n",
    "        'episode_durations': no_episode_durations,\n",
    "        'policy_losses': no_policy_losses,\n",
    "        'color': 'red'\n",
    "    }, 'Learned baseline': {\n",
    "        'episode_durations': learned_baseline_episode_durations,\n",
    "        'policy_losses': learned_baseline_policy_losses,\n",
    "        'color': 'green'\n",
    "    }, 'Self-Critic baseline': {\n",
    "        'episode_durations': sc_baseline_episode_durations,\n",
    "        'policy_losses': sc_baseline_policy_losses,\n",
    "        'color': 'blue'\n",
    "    }\n",
    "}\n",
    "\n",
    "compare_baselines_plot(baselines_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
